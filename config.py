import os
from pathlib import Path
import requests
import logging
from huggingface_hub import login

# Optional HF token for better reliability
HF_TOKEN = os.getenv("HUGGINGFACE_HUB_TOKEN") or os.getenv("HF_TOKEN")
if HF_TOKEN:
    try:
        login(token=HF_TOKEN)
        print("‚úÖ HuggingFace authentication successful")
    except Exception as e:
        print(f"‚ö†Ô∏è HF authentication failed: {e}")

# Set environment variables
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Detect if running on Hugging Face Spaces
IS_SPACES = os.getenv("SPACE_ID") is not None

# Use writable directories based on environment
if IS_SPACES:
    BASE_DIR = Path.home()
    MODEL_PATH = BASE_DIR / ".cache" / "models" / "BilingualModel.pt"
    OUTPUT_DIR = BASE_DIR / ".cache" / "output"
else:
    BASE_DIR = Path(__file__).parent
    MODEL_PATH = BASE_DIR / "models" / "BilingualModel.pt"
    OUTPUT_DIR = BASE_DIR / "output"

# Create required directories
for d in [MODEL_PATH.parent, OUTPUT_DIR / "images", OUTPUT_DIR / "crops", OUTPUT_DIR / "results"]:
    d.mkdir(parents=True, exist_ok=True)

def ensure_model_downloaded():
    """Download bilingual YOLO model if not present"""
    if not MODEL_PATH.exists():
        print(f"Downloading Bilingual YOLO model to {MODEL_PATH}...")
        # Updated URL for bilingual model
        url = "https://github.com/Barbatos101/newspaper-education-extractor/releases/download/v2.0/BilingualModel.pt"
        try:
            response = requests.get(url, stream=True, timeout=300)
            response.raise_for_status()
            total_size = int(response.headers.get('content-length', 0))
            with open(MODEL_PATH, 'wb') as f:
                downloaded = 0
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        if total_size > 0 and downloaded % (1024 * 1024) == 0:
                            percent = (downloaded / total_size) * 100
                            print(f"Download progress: {percent:.1f}%")
            print(f"‚úÖ Bilingual model downloaded successfully to {MODEL_PATH}")
            return True
        except Exception as e:
            print(f"‚ùå Error downloading bilingual model: {e}")
            print("‚ö†Ô∏è Continuing without model - some features may not work")
            return False
    return True

# Download model on import
model_available = ensure_model_downloaded()

# Updated settings with improved DPI and no page limits
CONFIDENCE_THRESHOLD = 0.82  # Higher confidence for bilingual model
KEYWORD_MIN_MATCH = 1  # Reduced for bilingual support
NUM_WORKERS = 1  # Single worker for stability
REDUCED_DPI = 180  # IMPROVED DPI from 120 to 180
MAX_PAGES_BATCH = 2  # Process 2 pages at a time for better memory management
MAX_INPUT_CHARS_FOR_SUMMARY = 400  # Slightly increased for better summaries
MAX_SUMMARY_LENGTH = 50  # Slightly longer summaries
SEMANTIC_ANALYSIS_ENABLED = False  # Disabled for performance
SAVE_CROPS_DEFAULT = False
MAX_PAGES_TO_PROCESS = None  # REMOVED PAGE LIMIT - Process all pages
ENABLE_QUICK_FILTER = True

print(f"üîß Using {NUM_WORKERS} workers for processing")
print(f"üåê Bilingual mode (English + Hindi)")
print(f"üìÑ DPI Quality: {REDUCED_DPI}")
print(f"üìë Page Processing: All pages (no limit)")
print(f"üéØ HF Spaces mode: {IS_SPACES}")

# Bilingual education keywords
EDUCATION_KEYWORDS_EN = [
    'school', 'schools', 'education', 'educational', 'student', 'students',
    'teacher', 'teachers', 'university', 'college', 'academic', 'classroom',
    'curriculum', 'exam', 'exams', 'graduation', 'scholarship', 'principal',
    'kindergarten', 'elementary', 'secondary', 'admission', 'enrollment',
    'learning', 'study', 'studies', 'degree', 'diploma', 'certificate'
]

EDUCATION_KEYWORDS_HI = [
    '‡§∏‡•ç‡§ï‡•Ç‡§≤', '‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§≤‡§Ø', '‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ', '‡§∂‡•à‡§ï‡•ç‡§∑‡§ø‡§ï', '‡§õ‡§æ‡§§‡•ç‡§∞', '‡§õ‡§æ‡§§‡•ç‡§∞‡§æ', '‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§∞‡•ç‡§•‡•Ä',
    '‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï', '‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§™‡§ï', '‡§µ‡§ø‡§∂‡•ç‡§µ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§≤‡§Ø', '‡§Æ‡§π‡§æ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§≤‡§Ø', '‡§ï‡•â‡§≤‡•á‡§ú', '‡§™‡§æ‡§†‡§∂‡§æ‡§≤‡§æ',
    '‡§™‡§æ‡§†‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ', '‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§æ', '‡§™‡•ç‡§∞‡§µ‡•á‡§∂', '‡§¶‡§æ‡§ñ‡§ø‡§≤‡§æ', '‡§õ‡§æ‡§§‡•ç‡§∞‡§µ‡•É‡§§‡•ç‡§§‡§ø', '‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§æ‡§ö‡§æ‡§∞‡•ç‡§Ø',
    '‡§™‡•ç‡§∞‡§æ‡§•‡§Æ‡§ø‡§ï', '‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ‡§ø‡§ï', '‡§â‡§ö‡•ç‡§ö', '‡§Ö‡§ß‡•ç‡§Ø‡§Ø‡§®', '‡§™‡§¢‡§º‡§æ‡§à', '‡§°‡§ø‡§ó‡•ç‡§∞‡•Ä', '‡§°‡§ø‡§™‡•ç‡§≤‡•ã‡§Æ‡§æ',
    '‡§™‡•ç‡§∞‡§Æ‡§æ‡§£‡§™‡§§‡•ç‡§∞', '‡§ï‡§ï‡•ç‡§∑‡§æ', '‡§ú‡•ç‡§û‡§æ‡§®', '‡§¨‡•ã‡§∞‡•ç‡§°', '‡§∏‡•Ä‡§¨‡•Ä‡§è‡§∏‡§à', '‡§Ü‡§à‡§∏‡•Ä‡§è‡§∏‡§à'
]

# Combined keywords for bilingual search
EDUCATION_KEYWORDS = EDUCATION_KEYWORDS_EN + EDUCATION_KEYWORDS_HI
CORE_EDUCATION_KEYWORDS = ['school', 'education', 'student', 'teacher', '‡§∏‡•ç‡§ï‡•Ç‡§≤', '‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ', '‡§õ‡§æ‡§§‡•ç‡§∞', '‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§≤‡§Ø']

# OCR configuration for bilingual support
OCR_LANG_EN = "eng"
OCR_LANG_HI = "hin"
OCR_LANG_BILINGUAL = "eng+hin"
OCR_PSM_PRIMARY = 6
OCR_PSM_FALLBACK = 4

# Ultra-lightweight models for HF Spaces
SUMMARIZATION_MODEL_EN = "sshleifer/distilbart-cnn-12-6"  # Smallest BART variant
SUMMARIZATION_MODEL_HI = None  # Use extractive summarization for Hindi
SEMANTIC_MODEL = "sentence-transformers/paraphrase-MiniLM-L3-v2"  # Smallest semantic model

SEMANTIC_THRESHOLD = 0.4

# Context exclusions (bilingual)
CONTEXT_EXCLUSIONS = [
    'weather', 'temperature', 'celsius', 'fahrenheit', '‡§Æ‡•å‡§∏‡§Æ', '‡§§‡§æ‡§™‡§Æ‡§æ‡§®',
    'clinical study', 'medical study', '‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ', '‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø',
    'stock market', 'financial report', '‡§∂‡•á‡§Ø‡§∞', '‡§¨‡§æ‡§ú‡§æ‡§∞',
    'sports score', 'match result', '‡§ñ‡•á‡§≤', '‡§Æ‡•à‡§ö'
]

# Processing timeouts - increased for all-page processing
PDF_PROCESSING_TIMEOUT = 300  # Increased for more pages
MODEL_INFERENCE_TIMEOUT = 15

# File size limits - slightly increased for better quality processing
MAX_FILE_SIZE_MB = 15  # Increased from 10MB to handle better DPI

# Language detection patterns
HINDI_PATTERNS = [
    r'[\u0900-\u097F]',  # Devanagari script
    r'[\u0980-\u09FF]',  # Bengali script (sometimes used)
]

ENGLISH_PATTERNS = [
    r'[a-zA-Z]+'
]
